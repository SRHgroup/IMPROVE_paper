{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime\n",
    "import xgboost\n",
    "\n",
    "import sklearn.ensemble as ensemble\n",
    "#from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRFClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from platform import python_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# costumized stuff  \n",
    "patient_data = False\n",
    "Simple =  True\n",
    "OnlyTME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the data with or without the patient data for tumor microenviroment and with or withput selected cols\n",
    "#print(\"running model with only_imp_cols:\" + only_imp_cols + \"and patient_data\"+ patient_data)\n",
    "\n",
    "if patient_data == True and Simple == False: \n",
    "    cols_to_drop = ['response','Peptide', 'HLA_allele','Patient','Partition','cohort','est_freq', 'Allele_Frequency',\n",
    "                   'rna_coef_100',\"variant_allele_frequency\",\n",
    "                'T cells','CD8 T cells','Cytotoxic lymphocytes','B lineage',\n",
    "                   'NK cells','Myeloid dendritic cells',\n",
    "                   'Neutrophils','Endothelial cells','Fibroblasts',\n",
    "                   \"DAI\"]\n",
    "    \n",
    "    \n",
    "                #   'Mut_MHCrank_EL', 'Expression_Level','DAI', 'X.Rank_Stab',  'foginess_score']\n",
    "    TME_inc = \"TME_included\"\n",
    "    sim = \"advance\"\n",
    "if patient_data == False and Simple == False: \n",
    "    cols_to_drop = ['response','Peptide', 'HLA_allele','Patient','Partition','cohort','Allele_Frequency',\n",
    "                    'est_freq','Allele_Frequency','rna_coef_100', \"variant_allele_frequency\",\n",
    "                   'T cells','CD8 T cells','Cytotoxic lymphocytes','B lineage',\"CYT\",\n",
    "                   'NK cells','Monocytic lineage','Myeloid dendritic cells','HLA_expression',\n",
    "                   'Neutrophils','Endothelial cells','Fibroblasts',\n",
    "                   \"DAI\"]\n",
    "                   # 'Mut_MHCrank_EL', 'Expression_Level','DAI', 'X.Rank_Stab',  'foginess_score']\n",
    "    TME_inc = \"TME_excluded\"\n",
    "    sim = \"advance\"\n",
    "if patient_data == False and Simple == True: \n",
    "    cols_to_drop = ['response','Peptide', 'HLA_allele','Patient','Partition','cohort','est_freq',\n",
    "                   \"Expression_Level\", \"cellular_prevalence\",\"CYT\",\"HLA_expression\",\"variant_allele_frequency\",\n",
    "                   \"priority_Score\",'Allele_Frequency',\n",
    "                   'T cells','CD8 T cells','Cytotoxic lymphocytes','B lineage',\n",
    "                   'NK cells','Monocytic lineage','Myeloid dendritic cells',\n",
    "                   'Neutrophils','Endothelial cells','Fibroblasts']\n",
    "    TME_inc = \"TME_excluded\"\n",
    "    sim = \"Simple\"\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = data[[\"response\",\"Peptide\", \"HLA_allele\",\"Patient\",\"Partition\",\"cohort\",\"est_freq\",\"CYT\",\"CD8A\",\n",
    " #             \"Prop_Hydrophobic\",\"Score_PRIME\",\"Mut_MHCrank_BA\",\"Mut_MHCrank_EL\",\"DAI\",\n",
    "  #          \"Thalf.h.\",\"mut_rep_rank_netstabpan\",\"agretopicity\",\"helix\",\"inst\",\"Prop_Polar\",\"aro\",\n",
    "   #         \"mw\",\"Prop_Non.polar\",\"Prop_Aliphatic\",\"Prop_Tiny\",\"Prop_Charged\",\"Prop_Small\",\"Prop_Basic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/03_partitioning_data/txt/03_2_filtered_data_model.txt', sep='\\t')\n",
    "Var_importance_filename = '../../results/RandomForrest/tabels/Feature_importance_' + TME_inc+sim +'_final.txt'\n",
    "#outfile = open(Var_importance_filename ,'w')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mw', 'aro', 'inst', 'cys_red', 'Mut_MHCrank_EL', 'Mut_MHCrank_BA',\n",
      "       'Expression_Level', 'Allele_Frequency', 'priority_Score', 'response',\n",
      "       'variant_allele_frequency', 'cellular_prevalence', 'Self_Similarity',\n",
      "       'est_freq', 'Score_PRIME', 'helix', 'MeanHydroph_coreNoAnc',\n",
      "       'Prop_Small', 'Prop_Aromatic', 'Prop_Basic', 'Prop_Acidic', 'pI', 'DAI',\n",
      "       'DAI_V2', 'X.Rank_Stab', 'rna_coef_100', 'NetMHCExp',\n",
      "       'foreignness_score', 'HLA_expression', 'CYT', 'T cells', 'CD8 T cells',\n",
      "       'Cytotoxic lymphocytes', 'B lineage', 'NK cells', 'Monocytic lineage',\n",
      "       'Myeloid dendritic cells', 'Neutrophils', 'Endothelial cells',\n",
      "       'Fibroblasts', 'Peptide', 'HLA_allele', 'Patient', 'Partition',\n",
      "       'cohort'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "response\n",
       "0    17053\n",
       "1      467\n",
       "Name: response, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.columns)\n",
    "data.groupby(['response'])['response'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../models/sim/TME_excluded/rf3_TME_excluded_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-972b9ff1b2bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# pickle dumb model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mpickle_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../models/sim/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTME_inc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/rf'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTME_inc\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../models/sim/TME_excluded/rf3_TME_excluded_0.pkl'"
     ]
    }
   ],
   "source": [
    "# nested cross validation\n",
    "outfile = open(Var_importance_filename ,'w')\n",
    "n_sample = 50\n",
    "partitioning_list = data.Partition.unique()\n",
    "pred_df = pd.DataFrame()\n",
    "for i in partitioning_list:\n",
    "    av_pred_rf = 0\n",
    "    for n in range(n_sample):  \n",
    "        test = data[data.Partition == i]\n",
    "        train = data[data.Partition != i]\n",
    "        info = test[[\"Peptide\", \"HLA_allele\",\"Patient\",\"cohort\"]]\n",
    "        train_pos = train[train.response==1]\n",
    "      #  train_neg = train[train.response==0]\n",
    "       # len(train_neg)\n",
    "        train_neg = train[train.response==0].sample(n=500, random_state=n) #, random_state=45\n",
    "        train = shuffle(pd.concat([train_pos, train_neg], axis=0)).reset_index(drop=True)\n",
    "        train = train[~train.Peptide.isin(test[\"Peptide\"])]\n",
    "        X_train = train.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        y_train = train[\"response\"]\n",
    "        feature_list = list(X_train.columns)\n",
    "        \n",
    "\n",
    "\n",
    "        # Instantiate the RF and the MLP # \n",
    "        rf = RandomForestClassifier(random_state = 42, max_depth = 6, min_samples_leaf = 6,\n",
    "                                    n_estimators = 2000, n_jobs=-1) #class_weight={0:1,1:3}\n",
    "\n",
    "                # Train the models on training data\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # pickle dumb model \n",
    "        pickle_model = '../../models/'+sim+'/'+TME_inc+'/rf' + str(i) + '_' + TME_inc +'_' + str(n) + '.pkl'\n",
    "        pickle.dump(rf, open(pickle_model, 'wb'))\n",
    "        \n",
    "        X_test = test.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        y_test = test[\"response\"]  \n",
    "        len(X_test)\n",
    "\n",
    "        #Get fetures importance per partition\n",
    "        importances = list(rf.feature_importances_)\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "        # Sort\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "     #   [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "      #  gather all feature importance \n",
    "        for pair in feature_importances:\n",
    "            outfile.write(str(pair[0]) + \"\\t\" + str(pair[1]) + \"\\t\" + str(i) + \"\\n\")\n",
    "\n",
    "        prediction_rf = rf.predict_proba(X_test)\n",
    "\n",
    "        \n",
    "        av_pred_rf += prediction_rf\n",
    "    \n",
    "    pred_val_rf = av_pred_rf/n_sample\n",
    "        \n",
    "    pred_val_rf = pd.DataFrame(pred_val_rf[:,1])\n",
    "    pred_val_rf.reset_index(drop=True, inplace=True)\n",
    "    mapping = {pred_val_rf.columns[0]: \"prediction_rf\"}\n",
    "    pred_val_rf = pred_val_rf.rename(columns=mapping)\n",
    "\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    info.reset_index(drop=True, inplace=True)\n",
    "    print(len(X_test))\n",
    "    ped_df = pd.concat([info,X_test,y_test,pred_val_rf],axis = 1)\n",
    "    ped_df[\"Partition\"] = i\n",
    "    pred_df = pred_df.append(ped_df)\n",
    "\n",
    "        \n",
    "#print(\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))    \n",
    "print(\"n sample:\",n_sample,\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))  \n",
    "outfile.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to outfile \n",
    "#currentDateTime = datetime.now().strftime(\"%m-%d-%Y %H-%M-%S %p\")\n",
    "pred_df.to_csv(r'../../results/RandomForrest/tabels/pred_df_'+TME_inc+sim+'_final.txt', index=None, sep=' ', mode='w')\n",
    "\n",
    "data_new = data\n",
    "cols = ['priority_Score','Prop_Small', 'Prop_Basic', 'Prop_Acidic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priority_Score\n",
      "Index(['mw', 'aro', 'inst', 'cys_red', 'Mut_MHCrank_EL', 'Mut_MHCrank_BA',\n",
      "       'Expression_Level', 'response', 'variant_allele_frequency',\n",
      "       'cellular_prevalence', 'Self_Similarity', 'est_freq', 'Score_PRIME',\n",
      "       'helix', 'MeanHydroph_coreNoAnc', 'Prop_Small', 'Prop_Aromatic',\n",
      "       'Prop_Basic', 'Prop_Acidic', 'pI', 'DAI', 'X.Rank_Stab', 'rna_coef_100',\n",
      "       'NetMHCExp', 'foginess_score', 'HLA_expression', 'CYT', 'T cells',\n",
      "       'CD8 T cells', 'Cytotoxic lymphocytes', 'B lineage', 'NK cells',\n",
      "       'Monocytic lineage', 'Myeloid dendritic cells', 'Neutrophils',\n",
      "       'Endothelial cells', 'Fibroblasts', 'Peptide', 'HLA_allele', 'Patient',\n",
      "       'Partition', 'cohort'],\n",
      "      dtype='object')\n",
      "2897\n",
      "3254\n",
      "3868\n",
      "6098\n",
      "3424\n",
      "n sample: 50 RF AUC: 0.6375\n",
      "Prop_Small\n",
      "Index(['mw', 'aro', 'inst', 'cys_red', 'Mut_MHCrank_EL', 'Mut_MHCrank_BA',\n",
      "       'Expression_Level', 'priority_Score', 'response',\n",
      "       'variant_allele_frequency', 'cellular_prevalence', 'Self_Similarity',\n",
      "       'est_freq', 'Score_PRIME', 'helix', 'MeanHydroph_coreNoAnc',\n",
      "       'Prop_Aromatic', 'Prop_Basic', 'Prop_Acidic', 'pI', 'DAI',\n",
      "       'X.Rank_Stab', 'rna_coef_100', 'NetMHCExp', 'foginess_score',\n",
      "       'HLA_expression', 'CYT', 'T cells', 'CD8 T cells',\n",
      "       'Cytotoxic lymphocytes', 'B lineage', 'NK cells', 'Monocytic lineage',\n",
      "       'Myeloid dendritic cells', 'Neutrophils', 'Endothelial cells',\n",
      "       'Fibroblasts', 'Peptide', 'HLA_allele', 'Patient', 'Partition',\n",
      "       'cohort'],\n",
      "      dtype='object')\n",
      "2897\n",
      "3254\n",
      "3868\n",
      "6098\n",
      "3424\n",
      "n sample: 50 RF AUC: 0.636\n",
      "Prop_Basic\n",
      "Index(['mw', 'aro', 'inst', 'cys_red', 'Mut_MHCrank_EL', 'Mut_MHCrank_BA',\n",
      "       'Expression_Level', 'priority_Score', 'response',\n",
      "       'variant_allele_frequency', 'cellular_prevalence', 'Self_Similarity',\n",
      "       'est_freq', 'Score_PRIME', 'helix', 'MeanHydroph_coreNoAnc',\n",
      "       'Prop_Small', 'Prop_Aromatic', 'Prop_Acidic', 'pI', 'DAI',\n",
      "       'X.Rank_Stab', 'rna_coef_100', 'NetMHCExp', 'foginess_score',\n",
      "       'HLA_expression', 'CYT', 'T cells', 'CD8 T cells',\n",
      "       'Cytotoxic lymphocytes', 'B lineage', 'NK cells', 'Monocytic lineage',\n",
      "       'Myeloid dendritic cells', 'Neutrophils', 'Endothelial cells',\n",
      "       'Fibroblasts', 'Peptide', 'HLA_allele', 'Patient', 'Partition',\n",
      "       'cohort'],\n",
      "      dtype='object')\n",
      "2897\n",
      "3254\n",
      "3868\n",
      "6098\n",
      "3424\n",
      "n sample: 50 RF AUC: 0.6345\n",
      "Prop_Acidic\n",
      "Index(['mw', 'aro', 'inst', 'cys_red', 'Mut_MHCrank_EL', 'Mut_MHCrank_BA',\n",
      "       'Expression_Level', 'priority_Score', 'response',\n",
      "       'variant_allele_frequency', 'cellular_prevalence', 'Self_Similarity',\n",
      "       'est_freq', 'Score_PRIME', 'helix', 'MeanHydroph_coreNoAnc',\n",
      "       'Prop_Small', 'Prop_Aromatic', 'Prop_Basic', 'pI', 'DAI', 'X.Rank_Stab',\n",
      "       'rna_coef_100', 'NetMHCExp', 'foginess_score', 'HLA_expression', 'CYT',\n",
      "       'T cells', 'CD8 T cells', 'Cytotoxic lymphocytes', 'B lineage',\n",
      "       'NK cells', 'Monocytic lineage', 'Myeloid dendritic cells',\n",
      "       'Neutrophils', 'Endothelial cells', 'Fibroblasts', 'Peptide',\n",
      "       'HLA_allele', 'Patient', 'Partition', 'cohort'],\n",
      "      dtype='object')\n",
      "2897\n",
      "3254\n",
      "3868\n",
      "6098\n",
      "3424\n",
      "n sample: 50 RF AUC: 0.6348\n"
     ]
    }
   ],
   "source": [
    "# nested cross validation\n",
    "outfile = open(Var_importance_filename ,'w')\n",
    "for col in cols:\n",
    "    print(col)\n",
    "    data = data_new.drop(col, axis=1).reset_index(drop=True)\n",
    "    print(data.columns)\n",
    "    n_sample = 50\n",
    "    partitioning_list = data.Partition.unique()\n",
    "    pred_df = pd.DataFrame()\n",
    "    for i in partitioning_list:\n",
    "        av_pred_rf = 0\n",
    "        for n in range(n_sample):  \n",
    "            test = data[data.Partition == i]\n",
    "            train = data[data.Partition != i]\n",
    "            info = test[[\"Peptide\", \"HLA_allele\",\"Patient\",\"cohort\"]]\n",
    "            train_pos = train[train.response==1]\n",
    "          #  train_neg = train[train.response==0]\n",
    "           # len(train_neg)\n",
    "            train_neg = train[train.response==0].sample(n=500, random_state=n) #, random_state=45\n",
    "            train = shuffle(pd.concat([train_pos, train_neg], axis=0)).reset_index(drop=True)\n",
    "            train = train[~train.Peptide.isin(test[\"Peptide\"])]\n",
    "            X_train = train.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "            y_train = train[\"response\"]\n",
    "            feature_list = list(X_train.columns)\n",
    "\n",
    "\n",
    "\n",
    "            # Instantiate the RF and the MLP # \n",
    "            rf = RandomForestClassifier(random_state = 42, max_depth = 6, min_samples_leaf = 6,\n",
    "                                        n_estimators = 2000, n_jobs=-1) #class_weight={0:1,1:3}\n",
    "\n",
    "                    # Train the models on training data\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            # pickle dumb model \n",
    "          #  pickle_model = '../../models/rf' + str(i) + '_'+ TME_inc +'_' + str(n) + '.pkl'\n",
    "          #  pickle.dump(rf, open(pickle_model, 'wb'))\n",
    "\n",
    "            X_test = test.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "            y_test = test[\"response\"]  \n",
    "            len(X_test)\n",
    "\n",
    "            #Get fetures importance per partition\n",
    "            importances = list(rf.feature_importances_)\n",
    "            feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "            # Sort\n",
    "            feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "         #   [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "          #  gather all feature importance \n",
    "            for pair in feature_importances:\n",
    "                outfile.write(str(pair[0]) + \"\\t\" + str(pair[1]) + \"\\t\" + str(i) + \"\\n\")\n",
    "\n",
    "            prediction_rf = rf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "            av_pred_rf += prediction_rf\n",
    "\n",
    "        pred_val_rf = av_pred_rf/n_sample\n",
    "\n",
    "        pred_val_rf = pd.DataFrame(pred_val_rf[:,1])\n",
    "        pred_val_rf.reset_index(drop=True, inplace=True)\n",
    "        mapping = {pred_val_rf.columns[0]: \"prediction_rf\"}\n",
    "        pred_val_rf = pred_val_rf.rename(columns=mapping)\n",
    "\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        info.reset_index(drop=True, inplace=True)\n",
    "        print(len(X_test))\n",
    "        ped_df = pd.concat([info,X_test,y_test,pred_val_rf],axis = 1)\n",
    "        ped_df[\"Partition\"] = i\n",
    "        pred_df = pred_df.append(ped_df)\n",
    "\n",
    "\n",
    "    #print(\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))    \n",
    "    print(\"n sample:\",n_sample,\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))  \n",
    "    \n",
    "outfile.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 3, 2])"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioning_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_Score 0.6377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to outfile \n",
    "pred_df.to_csv(r'../../results/RandomForrest/tabels/pred_df_'+TME_inc+sim+'.txt', index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Peptide  HLA_allele  Patient   cohort         mw       aro  \\\n",
      "0     ILEYTDQISKY  HLA-A01:01  BC-1849  bladder  1372.5186  0.181818   \n",
      "1       QLEQLMQLY  HLA-A01:01  BC-1849  bladder  1165.3589  0.111111   \n",
      "2      LLDSTILEYT  HLA-A01:01  BC-1849  bladder  1167.3054  0.100000   \n",
      "3      DADLLRPHAY  HLA-A01:01  BC-9517  bladder  1170.2745  0.100000   \n",
      "4      DADLLRPHAY  HLA-B35:01  BC-9517  bladder  1170.2745  0.100000   \n",
      "...           ...         ...      ...      ...        ...       ...   \n",
      "3419    SQVFPHLSL  HLA-B40:01    RH-33   Basket  1027.1736  0.111111   \n",
      "3420    SQVFPHLSL  HLA-C03:04    RH-33   Basket  1027.1736  0.111111   \n",
      "3421    SQVFPHLSL  HLA-C07:02    RH-33   Basket  1027.1736  0.111111   \n",
      "3422  ALIGVSVAWIL  HLA-A02:01    RH-33   Basket  1141.4020  0.090909   \n",
      "3423    VSVAWILVL  HLA-C03:04    RH-33   Basket   999.2462  0.111111   \n",
      "\n",
      "           inst  cys_red  Mut_MHCrank_EL  Mut_MHCrank_BA  ...  Prop_Aromatic  \\\n",
      "0     18.881818     2980          0.2636          0.6203  ...       0.142857   \n",
      "1     21.911111     1490          0.0784          0.1517  ...       0.000000   \n",
      "2     39.030000     1490          0.3495          0.2507  ...       0.000000   \n",
      "3     39.030000     1490          0.2137          0.4873  ...       0.166667   \n",
      "4     39.030000     1490          0.3832          0.7888  ...       0.166667   \n",
      "...         ...      ...             ...             ...  ...            ...   \n",
      "3419  43.311111        0          0.5846          0.5873  ...       0.400000   \n",
      "3420  43.311111        0          0.4186          0.5259  ...       0.400000   \n",
      "3421  43.311111        0          0.6425          1.1888  ...       0.400000   \n",
      "3422  26.600000     5500          1.3341          1.7055  ...       0.142857   \n",
      "3423  30.288889     5500          0.5027          0.3104  ...       0.200000   \n",
      "\n",
      "      Prop_Basic  Prop_Acidic        pI      DAI  X.Rank_Stab  foginess_score  \\\n",
      "0       0.142857     0.142857  4.184396   0.0252          1.3             1.0   \n",
      "1       0.000000     0.000000  3.849974  -0.0560          1.2             1.0   \n",
      "2       0.000000     0.200000  3.550064   0.0000         20.0             1.0   \n",
      "3       0.333333     0.000000  5.408066  19.0177          1.3             1.0   \n",
      "4       0.333333     0.000000  5.408066  15.2297          6.0             1.0   \n",
      "...          ...          ...       ...      ...          ...             ...   \n",
      "3419    0.200000     0.000000  7.550325  -0.0231          1.1             1.0   \n",
      "3420    0.200000     0.000000  7.550325   0.1056         13.0             1.0   \n",
      "3421    0.200000     0.000000  7.550325  -0.1152          8.0             1.0   \n",
      "3422    0.000000     0.000000  6.099982   7.9326          1.1             1.0   \n",
      "3423    0.000000     0.000000  6.099982  -0.2159         11.0             1.0   \n",
      "\n",
      "      response  prediction_rf  Partition  \n",
      "0            1       0.023579          4  \n",
      "1            1       0.038611          4  \n",
      "2            0       0.025357          4  \n",
      "3            1       0.018794          4  \n",
      "4            0       0.014843          4  \n",
      "...        ...            ...        ...  \n",
      "3419         0       0.020512          2  \n",
      "3420         0       0.024715          2  \n",
      "3421         0       0.017031          2  \n",
      "3422         0       0.052923          2  \n",
      "3423         0       0.120697          2  \n",
      "\n",
      "[19541 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904\n",
      "3904\n",
      "3903\n",
      "3903\n",
      "3910\n",
      "3910\n",
      "3917\n",
      "3917\n",
      "3907\n",
      "3907\n",
      "md: 2 RF AUC: 0.662\n",
      "3904\n",
      "3904\n",
      "3903\n",
      "3903\n",
      "3910\n",
      "3910\n",
      "3917\n",
      "3917\n",
      "3907\n",
      "3907\n",
      "md: 5 RF AUC: 0.662\n",
      "3904\n",
      "3904\n",
      "3903\n",
      "3903\n",
      "3910\n",
      "3910\n",
      "3917\n",
      "3917\n",
      "3907\n",
      "3907\n",
      "md: 10 RF AUC: 0.662\n"
     ]
    }
   ],
   "source": [
    "for md in [2, 5, 10]:\n",
    "    partitioning_list = data.Partition.unique()\n",
    "    pred_df = pd.DataFrame()\n",
    "    for i in partitioning_list:\n",
    "        test = data[data.Partition == i]\n",
    "        train = data[data.Partition != i]\n",
    "        info = test[[\"Peptide\", \"HLA_allele\",\"Patient\",\"cohort\"]]\n",
    "        print(len(info))\n",
    "       # train_pos = train[train.response==1]\n",
    "       # train_neg = train[train.response==0].sample(n=2000, random_state=n) #, random_state=45\n",
    "       # train = shuffle(pd.concat([train_pos, train_neg], axis=0)).reset_index(drop=True)\n",
    "        X_train = train.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        y_train = train[\"response\"]\n",
    "        feature_list = list(X_train.columns)\n",
    "        \n",
    "        space = dict()\n",
    "        space['n_estimators'] = [10, 100, 500]\n",
    "        space['max_features'] = [2, 4, 6]\n",
    "      \n",
    "\n",
    "\n",
    "        # Instantiate the RF and the MLP\n",
    "        rf = RandomForestClassifier(random_state = 42, min_samples_leaf = 6,\n",
    "                                    max_depth = 6, n_estimators = 2000, \n",
    "                                     n_jobs=-1) \n",
    "        \n",
    "        search = GridSearchCV(model, space, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "        result = search.fit(X_train, y_train)\n",
    "        best_model = result.best_estimator_\n",
    "        \n",
    "       # rf = RFE(rf,n_features_to_select=5)\n",
    "\n",
    "                # Train the models on training data\n",
    "        rf.fit(X_train, y_train)\n",
    "        X_test = test.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        y_test = test[\"response\"]  \n",
    "\n",
    "        #Get fetures importance per partition\n",
    "        importances = list(rf.feature_importances_)\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "        # Sort\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "     #   [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "      #  gather all feature importance \n",
    "        for pair in feature_importances:\n",
    "            outfile.write(str(pair[0]) + \"\\t\" + str(pair[1]) + \"\\t\" + str(i) + \"\\n\")\n",
    "\n",
    "        prediction_rf = rf.predict_proba(X_test)\n",
    "\n",
    "        pred_val_rf = pd.DataFrame(prediction_rf[:,1])\n",
    "        pred_val_rf.reset_index(drop=True, inplace=True)\n",
    "        mapping = {pred_val_rf.columns[0]: \"prediction_rf\"}\n",
    "        pred_val_rf = pred_val_rf.rename(columns=mapping)\n",
    "\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        info.reset_index(drop=True, inplace=True)\n",
    "        print(len(X_test))\n",
    "        ped_df = pd.concat([info,X_test,y_test,pred_val_rf],axis = 1)\n",
    "        ped_df[\"Partition\"] = i\n",
    "        pred_df = pred_df.append(ped_df)\n",
    "\n",
    "\n",
    "    print(\"md:\",md,\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))    \n",
    "outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-107-ad98ff1dc060>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-107-ad98ff1dc060>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sudo pip install xgboost\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2897\n",
      ">acc=0.964, est=0.975, cfg={'max_features': 3, 'n_estimators': 1000}\n",
      "2897\n",
      "3254\n",
      ">acc=0.968, est=0.974, cfg={'max_features': 3, 'n_estimators': 1000}\n",
      "3254\n",
      "3868\n",
      ">acc=0.973, est=0.974, cfg={'max_features': 3, 'n_estimators': 1000}\n",
      "3868\n",
      "6098\n",
      ">acc=0.983, est=0.969, cfg={'max_features': 3, 'n_estimators': 1000}\n",
      "6098\n",
      "3424\n",
      ">acc=0.970, est=0.974, cfg={'max_features': 3, 'n_estimators': 1000}\n",
      "3424\n",
      "RF AUC: 0.6169\n"
     ]
    }
   ],
   "source": [
    "# nested cross validation\n",
    "partitioning_list = data.Partition.unique()\n",
    "pred_df = pd.DataFrame()\n",
    "for i in partitioning_list:\n",
    "    test = data[data.Partition == i]\n",
    "    train = data[data.Partition != i]\n",
    "    info = test[[\"Peptide\", \"HLA_allele\",\"Patient\",\"cohort\"]]\n",
    "    print(len(info))\n",
    "   # train_pos = train[train.response==1]\n",
    "   # train_neg = train[train.response==0].sample(n=2000, random_state=n) #, random_state=45\n",
    "   # train = shuffle(pd.concat([train_pos, train_neg], axis=0)).reset_index(drop=True)\n",
    "    X_train = train.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "    y_train = train[\"response\"]\n",
    "    feature_list = list(X_train.columns)\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    rf = RandomForestClassifier(random_state = 42, \n",
    "                                    max_depth = 6, min_samples_leaf = 6,\n",
    "                                n_estimators = 2000, \n",
    "                                     n_jobs=-1)  \n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['n_estimators'] = [1000,10, 100, 500,1000]\n",
    "    space['max_features'] = [3,2, 3,4,5]\n",
    "    \n",
    "    search = GridSearchCV(rf, space, scoring='accuracy', cv=cv_inner, refit=True)\n",
    "    result = search.fit(X_train, y_train)\n",
    "    best_model = result.best_estimator_\n",
    "   \n",
    "    # Instantiate the RF and the MLP # \n",
    "    \n",
    "   # rf = RFE(rf,n_features_to_select=5)\n",
    "\n",
    "            # Train the models on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    X_test = test.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "    y_test = test[\"response\"]  \n",
    "    \n",
    "    yhat = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, yhat)\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' % (acc, result.best_score_, result.best_params_))\n",
    "\n",
    "    #Get fetures importance per partition\n",
    "    importances = list(rf.feature_importances_)\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    " #   [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "  #  gather all feature importance \n",
    "    for pair in feature_importances:\n",
    "        outfile.write(str(pair[0]) + \"\\t\" + str(pair[1]) + \"\\t\" + str(i) + \"\\n\")\n",
    "\n",
    "    prediction_rf = rf.predict_proba(X_test)\n",
    "\n",
    "    pred_val_rf = pd.DataFrame(prediction_rf[:,1])\n",
    "    pred_val_rf.reset_index(drop=True, inplace=True)\n",
    "    mapping = {pred_val_rf.columns[0]: \"prediction_rf\"}\n",
    "    pred_val_rf = pred_val_rf.rename(columns=mapping)\n",
    "\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    info.reset_index(drop=True, inplace=True)\n",
    "    print(len(X_test))\n",
    "    ped_df = pd.concat([info,X_test,y_test,pred_val_rf],axis = 1)\n",
    "    ped_df[\"Partition\"] = i\n",
    "    pred_df = pred_df.append(ped_df)\n",
    "\n",
    "        \n",
    "print(\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))    \n",
    "outfile.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">acc=0.964, est=0.975, cfg={'max_features': 6, 'n_estimators': 2000}\n",
    "2897\n",
    "3254\n",
    ">acc=0.968, est=0.974, cfg={'max_features': 6, 'n_estimators': 2000}\n",
    "3254\n",
    "3868\n",
    ">acc=0.973, est=0.974, cfg={'max_features': 6, 'n_estimators': 2000}\n",
    "3868\n",
    "6098\n",
    ">acc=0.983, est=0.969, cfg={'max_features': 6, 'n_estimators': 2000}\n",
    "6098\n",
    "3424\n",
    ">acc=0.970, est=0.974, cfg={'max_features': 6, 'n_estimators': 2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_cols = ['mw', 'aro', 'inst', 'cys_red', 'Mut_MHCrank_EL', 'Mut_MHCrank_BA',\n",
    "       'Expression_Level', 'priority_Score', \n",
    "       'variant_allele_frequency', 'cellular_prevalence', 'Self_Similarity',\n",
    "       'Score_PRIME', 'helix', 'MeanHydroph_coreNoAnc',\n",
    "       'Prop_Small', 'Prop_Aromatic', 'Prop_Basic', 'Prop_Acidic', 'pI', 'DAI',\n",
    "       'X.Rank_Stab', 'foginess_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mw\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6293\n",
      "aro\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6285\n",
      "inst\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6283\n",
      "cys_red\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6259\n",
      "Mut_MHCrank_EL\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6284\n",
      "Mut_MHCrank_BA\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6263\n",
      "Expression_Level\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6257\n",
      "priority_Score\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.629\n",
      "variant_allele_frequency\n",
      "2897\n",
      "2897\n",
      "3254\n",
      "3254\n",
      "3868\n",
      "3868\n",
      "6098\n",
      "6098\n",
      "3424\n",
      "3424\n",
      "RF AUC: 0.6292\n",
      "cellular_prevalence\n",
      "2897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-685-8fa1b27361ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Train the models on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols_to_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# nested cross validation\n",
    "for col in dat_cols:\n",
    "    print(col)\n",
    "    partitioning_list = data.Partition.unique()\n",
    "    pred_df = pd.DataFrame()\n",
    "    for i in partitioning_list:\n",
    "        test = data[data.Partition == i]\n",
    "        train = data[data.Partition != i]\n",
    "        train = train[~train.Peptide.isin(test[\"Peptide\"])]\n",
    "        info = test[[\"Peptide\", \"HLA_allele\",\"Patient\",\"cohort\"]]\n",
    "        print(len(info))\n",
    "       # train_pos = train[train.response==1]\n",
    "       # train_neg = train[train.response==0].sample(n=2000, random_state=n) #, random_state=45\n",
    "       # train = shuffle(pd.concat([train_pos, train_neg], axis=0)).reset_index(drop=True)\n",
    "        X_train = train.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        X_train = X_train.drop(col, axis=1).reset_index(drop=True)\n",
    "        y_train = train[\"response\"]\n",
    "        feature_list = list(X_train.columns)\n",
    "\n",
    "\n",
    "        # Instantiate the RF and the MLP # \n",
    "        rf = RandomForestClassifier(random_state = 42, max_depth = 6, min_samples_leaf = 6,\n",
    "                                    n_estimators = 2000, n_jobs=-1)\n",
    "                                  #  class_weight = \"balanced_subsample\", )  \n",
    "       # rf = BalancedRandomForestClassifier(random_state = 42, max_depth = 6, min_samples_leaf = 6,\n",
    "                                 #   n_estimators = 2000, n_jobs=-1)  \n",
    "      #  rf = XGBRFClassifier(n_estimators=500, subsample=0.9, colsample_bynode=0.2)\n",
    "       # rf = RFE(rf,n_features_to_select=5)\n",
    "\n",
    "                # Train the models on training data\n",
    "        rf.fit(X_train, y_train)\n",
    "        X_test = test.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        X_test = X_test.drop(col, axis=1).reset_index(drop=True)\n",
    "        y_test = test[\"response\"]  \n",
    "\n",
    "        #Get fetures importance per partition\n",
    "        importances = list(rf.feature_importances_)\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "        # Sort\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "     #   [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "      #  gather all feature importance \n",
    "        for pair in feature_importances:\n",
    "            outfile.write(str(pair[0]) + \"\\t\" + str(pair[1]) + \"\\t\" + str(i) + \"\\n\")\n",
    "\n",
    "        prediction_rf = rf.predict_proba(X_test)\n",
    "\n",
    "        pred_val_rf = pd.DataFrame(prediction_rf[:,1])\n",
    "        pred_val_rf.reset_index(drop=True, inplace=True)\n",
    "        mapping = {pred_val_rf.columns[0]: \"prediction_rf\"}\n",
    "        pred_val_rf = pred_val_rf.rename(columns=mapping)\n",
    "\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "        info.reset_index(drop=True, inplace=True)\n",
    "        print(len(X_test))\n",
    "        ped_df = pd.concat([info,X_test,y_test,pred_val_rf],axis = 1)\n",
    "        ped_df[\"Partition\"] = i\n",
    "        pred_df = pred_df.append(ped_df)\n",
    "\n",
    "\n",
    "    print(\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))    \n",
    "outfile.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2897\n",
      "3254\n",
      "3868\n",
      "6098\n",
      "3424\n",
      "n sample: 10 RF AUC: 0.658\n"
     ]
    }
   ],
   "source": [
    "# nested cross validation\n",
    "n_sample = 10\n",
    "partitioning_list = data.Partition.unique()\n",
    "pred_df = pd.DataFrame()\n",
    "for i in partitioning_list:\n",
    "    av_pred_rf = 0\n",
    "    for n in range(n_sample):  \n",
    "        test = data[data.Partition == i]\n",
    "        train = data[data.Partition != i]\n",
    "        info = test[[\"Peptide\", \"HLA_allele\",\"Patient\",\"cohort\"]]\n",
    "        train_pos = train[train.response==1]\n",
    "      #  train_neg = train[train.response==0]\n",
    "       # len(train_neg)\n",
    "        train_neg = train[train.response==0].sample(n=500, random_state=n) #, random_state=45\n",
    "        train = shuffle(pd.concat([train_pos, train_neg], axis=0)).reset_index(drop=True)\n",
    "        train = train[~train.Peptide.isin(test[\"Peptide\"])]\n",
    "        X_train = train.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        y_train = train[\"response\"]\n",
    "        feature_list = list(X_train.columns)\n",
    "        \n",
    "\n",
    "\n",
    "        # Instantiate the RF and the MLP # \n",
    "        rf = RandomForestClassifier(random_state = 42, max_depth = 3, min_samples_leaf = 6,\n",
    "                                    n_estimators = 2000, n_jobs=-1) #class_weight={0:1,1:3}\n",
    "\n",
    "                # Train the models on training data\n",
    "        rf.fit(X_train, y_train)\n",
    "        X_test = test.drop(cols_to_drop, axis=1).reset_index(drop=True)\n",
    "        y_test = test[\"response\"]  \n",
    "        len(X_test)\n",
    "\n",
    "        #Get fetures importance per partition\n",
    "        importances = list(rf.feature_importances_)\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "        # Sort\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "     #   [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "      #  gather all feature importance \n",
    "        for pair in feature_importances:\n",
    "            outfile.write(str(pair[0]) + \"\\t\" + str(pair[1]) + \"\\t\" + str(i) + \"\\n\")\n",
    "\n",
    "        prediction_rf = rf.predict_proba(X_test)\n",
    "\n",
    "        \n",
    "        av_pred_rf += prediction_rf\n",
    "    \n",
    "    pred_val_rf = av_pred_rf/n_sample\n",
    "        \n",
    "    pred_val_rf = pd.DataFrame(pred_val_rf[:,1])\n",
    "    pred_val_rf.reset_index(drop=True, inplace=True)\n",
    "    mapping = {pred_val_rf.columns[0]: \"prediction_rf\"}\n",
    "    pred_val_rf = pred_val_rf.rename(columns=mapping)\n",
    "\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    info.reset_index(drop=True, inplace=True)\n",
    "    print(len(X_test))\n",
    "    ped_df = pd.concat([info,X_test,y_test,pred_val_rf],axis = 1)\n",
    "    ped_df[\"Partition\"] = i\n",
    "    pred_df = pred_df.append(ped_df)\n",
    "\n",
    "        \n",
    "#print(\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))    \n",
    "print(\"n sample:\",n_sample,\"RF AUC:\",round(roc_auc_score(pred_df.response, pred_df.prediction_rf),4))  \n",
    "outfile.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to outfile \n",
    "pred_df.to_csv(r'../../results/RandomForrest/tabels/pred_df_'+TME_inc+sim+'.txt', index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-804-a49a8cc37dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "n_sample = [2,4,6,8,10,12,14,16,18,20]\n",
    "print(n_sample)\n",
    "range(n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
